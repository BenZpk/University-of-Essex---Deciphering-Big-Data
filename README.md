### University of Essex - Deciphering Big Data e-Portfolio by Ben Zapka
## Unit 1:
Sarkar and Roychowdhury (2019) associate volume, velocity and variety to big data. They give an introduction to data types in Python like lists, sets, strings, tuples and dictionaries which are all important when implementing big data architectures. Furthermore, the first collaborative discussion started. It considered advantages and disadvantages of the Internet of Things, which played an important role in the emergence of big data technologies as it introduced a magnitude of data sources with huge amounts of data.

## Unit 2:
Sarkar and Roychowdhury (2019) as well as Kazil and Jarmul (2016) introduce advanced file handling and data manipulation libraries in Python: NumPy, Pandas and Matplotlib. In practical examples, they show how the learned techniques and the libraries simplify the writing of code for manipulation and mathematical operations with data in tabular format in practice. These libraries enable Data Scientists to directly perform operations on data in tabular form which is easy to understand and conduct. Engaging with the works of my peers within the collaborative discussion led to a deeper understanding of the IoT as others brought up points I did not think about before. The seminar explained different data files and the implications they hold and then dived a bit deeper into databases. An example of a database design was shown to give a better understanding of this process.

## Unit 3:
The lecture cast showed that as big data is defined, among other aspects, by the variety of data, the ways of collecting and storing also vary widely. Different types of data storage as well as different ways of data collection were introduced. While the optimal storage option is to be decided by the given data formats and their fit to different storage options, the way how data is collected is mainly dependent on how the data is available and often not in control of the Data Scientist. Sarkar and Roychowdhury (2019) further explained how data wrangling with Python works and how to handle a variety of data sources. Data wrangling is the process of turning raw data into data that can then be used for analysis. It includes, e.g., the filtering for missing values and outliers, the concatenation of different data sources or the grouping of data. Data sources need to be imported into Python for further processing and analysis. In a web scraping exercise, I wrote a script in Python that uses the word “Data Science” and parses it into a JSON file. The resulting code and JSON file can be accessed here (link to be included). Lastly, this unit also contained the wrap-up of the first collaborative discussion. The whole collaborative discussion can be accessed here (link to be included).

## Unit 4:
The lecture cast further explained the data management pipeline and then focused on one important step of this pipeline: data cleaning. In an exercise, the lecture cast showed the process of cleaning data with the help of an example by Kazil and Jarmul (2016). I followed along the example and documented my results here (link to be inserted). The reading included the works of Sarkar and Roychowdhury (2019), Kazil and Jarmul (2016) as well as Huxley et al. (2020). They elaborate on how to best perform data cleaning using Python. Furthermore, an even deeper examination of web scraping techniques was presented. This way, reading the literature and putting the newly acquired skills to practice in the exercises helped develop real-world skills in data cleaning and getting a first impression of possible complications that iterative process brings. A Data Management Pipeline Test additionally tested the theoretical knowledge necessary for working with data.

## Unit 5:
Unit 5 considered the opportunities the usage of APIs can bring to the data cleaning process, e.g., by automating important process steps. The literature by IBM Intellas (2016) and Datanami (2019) introduced ways of automating parts of the data pipeline. They introduced the importance of Extract, Transform, Load (ETL) pipelines which extract data from different sources, transform it so that all information are presented in the same format and then load it into a final destination, e.g., a data warehouse in which the data can be further processed and analysed. By automating these process steps, time is saved and a more efficient way of working is created. Furthermore, human mistakes in manual process steps are eliminated and employee satisfaction increases via automation of unpopular manual process steps.

## Unit 6:
A lecture cast and the readings of this unit defined data normalisation and explained its importance. The first three normal forms are introduced. Normalisation is especially important because it helps avoid redundancy and mistakes while ensuring data integrity. By splitting up bigger data tables into smaller sub-tables that are linked, the performance of data base operations is increased. Using a relational database, e.g., makes a normalised database necessary and enables the user to query the data base with SQL. Data in different tables can be merged by join operations and analytics are made possible while optimising storage and speed of operations. [Sarkar and Roychowdhury (2019)]

For a graded assignment, in units 4 through 6, I collaborated in a group of three to create a logical database design for an online book store. We had one group meeting in the beginning where we split-up the tasks for the first part of the documentation, the definition of how our database will look like, among the three of us. Then, after a bit more than one week we met again to consider who puts together the three pieces, who implements the critical assessment of our solution and who proof-reads the whole piece. The meeting records of our two meetings where we have written the tasks of each individual of the group are linked here (link to be inserted). The final logical implementation of the database which we uploaded and got graded on is linked here (link to be inserted).


## Unit 7:
This unit’s readings provided more depth on database design both with SQL and NoSQL solutions, explaining strengths and weaknesses of both. While relational databases based on SQL profit from a good structure and easy queries, NoSQL infrastructures have the advantage of faster query times and horizontal scaling. Thus, it depends on the given use case which type of database should be chosen. In two assignments in this unit, the theoretical understanding of database normalisation and implementation via SQL was put into practice. In a first step, a table was put into first, second and third normal form to show the different steps in normalising a table to create a relational database. In a second step, this relational database was implemented physically by using SQL commands and then tested for referential integrity. This second step ensured that the process of coding databases with the use of SQL was understood as well and not only the theoretical parts of it. The normalised data table can be accessed here (link to be inserted) and the SQL files for the physical implementation of the relational database can be accessed here (link to be inserted). In the seminar, the application of NoSQL in practice was discussed, further increasing my understanding of these important considerations. [Letkowski (2015), IBM Intellas. (2010), Taipalus (2024) and Chen (2023)]

## Unit 8:
Due to the lecture cast, the importance of compliance with regulatory obligations lays not only in the fact that companies avoid fines but also that they avoid the loss of reputation, which is often even more important. The readings of the unit applied the concept of regulatory compliance on our topic of big data management and related database solutions. Here, security issues might lead to, e.g., major data breaches and resulting costs. Thus, complying to the obligations of, e.g., the General Data Protection Regulation (GDPR) is important. However, compliance alone will not be sufficient to protect against all possible security risks and is a first important step. The GDPR defines guardrails to follow to ensure transparency, purpose limitation, data minimisation, accuracy, storage limitation, integrity and accountability of data management. The second collaborative discussion started which circles around a comparison of the GDPR, which presents the data protection regulation on the EU-level, with a national data protection law to see minor differences in the approaches. The seminar showed the lifecycle of a property management database, also discussing that nowadays while SQL is still important most likely at least parts of big data databases are NoSQL-based. [Williams (2017) and Wired (2020)]

## Unit 9:
The lecture cast explored different database models before presenting the relational model which can be implemented using SQL in a bit more detail. The Entity-Relationship-Diagram was presented which can be used to define the interrelationships of different tables before implementing the database. This way a maximum referential integrity is ensured. Zoiner (2024) presents different big data architectures and how they can be put together dependent on the given use case. This shows how important it is to choose the correct components for the given use case to ensure for efficient processing and storage of data. In the seminar, different versions of how a database management system can be implemented using both SQL and NoSQL were presented, showcasing the differences and when each would be preferrable over the others.

## Unit 10:
Cooksey (2014) presents different API access methodologies. Especially, he makes a differentiation between polling and webhooks. Polling lets the client ask for updates. This approach, however, is rather inefficient as most likely there will be no updates available leading to a waste of resources. This issue is mitigated by long polling that lets the server wait until there is an update and only then answer the client. Webhooks, on the other side, effectively turn the client to a server and this way let the server deliver updates automatically to the client in return to a defined URL. This process can be further facilitated by introducing subscriptions and defining when the server can stop sending updates to the client. Connolly and Begg (2015) in a case study show how a database management system of a property management company can be implemented.

In this unit I finalised the second collaborative discussion by writing a summary post. The complete discussion can be accessed here (link to be inserted).

Furthermore, I also uploaded a definition of API security requirements for the X API that enables developers to connect to publicly available posts in X. This can be accessed here (link to be inserted).

## Unit 11:
Sarkar and Roychowdhury (2019) present a repetition of the concept of a relational database and some important SQL commands. Then, they give an example of how to connect SQLite to Python and integrate it into a Pandas DataFrame. While SQL is a powerful tool for database management, we will typically conduct our data wrangling steps and the analysis in Python. Furthermore, they also present a real world data wrangling exercise that shows how cleaning and merging data from multiple sources and storing the resulting data frame in SQL is achieved in a real world scenario. Cove (2020) shows how to visualise a data analysis by using ArcGIS Insights considering COVID 19 trends. This approach presents an additional way of how to present geographical data in an easy-to-understand way. An additional note based on IBM (2024) explains transactions with regards to databases and the properties of the ACID state. Furthermore, it explains how to enforce the ACID properties and stresses the importance of backups.

In this unit, the physical implementation of the logical database design presented in unit 6 was due. The files for the physical database design can be accessed here (link to be inserted). Some deviations from the logical design in unit 6 are explained.

Furthermore, I compared the grandfather-father-son backup procedure to alternatives and considered its usage in practice. The considerations for this task can be accessed here (link to be inserted).


## Unit 12:
The lecture cast presents emerging trends in databases, comprising the ideas behind different new database options that share a general trend towards the inclusion of different data types into the database design. Furthermore, it also introduces application fields for machine learning.

In the readings, Ethem (2010) gives an introduction to machine learning and Williams (2017) dives deeper into security and compliance topics related to it. Bhatnagar and Gajjar (2024) present new policy implications that exist due to the emergence of artificial intelligence. Special considerations for further policy in this field named are that all major decisions should be subject to human review, ensuring that the AI does not make decisions on its on but solely is used to provide information that are then used by human beings to inform their decision-making. This could make a ban on automated decision-making necessary. Furthermore, due to security implications, a ban on live facial recognition could be deemed necessary. Furthermore, the importance to make underlying AI code and related documentation publicly available is stressed to increase the transparency of AI developments. Impact assessments and audits of AI algorithms should take place to provide more trust into the solutions. Also, an opt-in or opt-out model could be used to give greater control to creatives over their works appearing in generative AI datasets and mitigate copyright infringements.
 
## Reflection:
I work as consultant in the field of AI transformations and thus already had a basis on some topics covered in this module beforehand. However, this module still managed to get me acquainted with some additional, important aspects. The hands-on coding of a database in SQL is something I was not used to before the module. Throughout the module I got the opportunity to improve my skills in implementing relational databases in SQL. In unit 7, I chose to create the SQL code myself to further improve my learnings whereas in unit 11’s individual graded assignment I chose to explore the opportunities MySQL’s forward engineering feature brings. Thus, my skills in SQL highly profited from my ability to work through several examples of how SQL works which were then put in practice. Throughout the module, together with improving my SQL skills, I got a better understanding of the logical and physical database design which I put into practice in two graded assignments to put my newly learned theoretical skills into practice and gain additional experience and confidence in their use.

Especially in the domain of data wrangling I was able to make progress throughout the module. Students are enabled to first work through literature considering the different techniques throughout the data wrangling pipeline to then put these learned techniques into practice. I produced code for web scraping in unit 3, explore how to clear a raw data set in unit 4 and normalise data to then implement into an SQL database in unit 7. I was rather unexperienced with some of these process steps and able to gain experience and comfort. Especially as web scraping and data cleaning together make up a majority of the data analysis pipeline, applying the learned concepts is of great importance. 

The module reinforced my understanding of the power that important Python libraries for data science hold. BeautifulSoup is powerful for web scraping, a technique commonly used to attain data sets that are available on websites. The usage of this library enabled me to solve a scraping exercise in unit 3 and will also be of great use in future tasks in my job, expanding the number of data sets that become available as not all useful information is available for easy download. NumPy and Pandas provide me with abilities to explore and clean the data set prior to the analysis as well as with capabilities for data modelling and analysis itself. These libraries came into practice in the unit 4 data cleaning exercise. While I already knew about the power of these libraries, it was good to keep my skills with them fresh and put them into practice. Furthermore, it was especially important to get an overview into how Python and SQL can be combined in unit 11. While Python is powerful for data cleaning and analysis, SQL is powerful for data storage and easy querying. In a company environment, data is not only downloaded, used for analysis and then deleted once the analytical results were published. Data is a valuable resource that can be used by many departments of a company. Thus, the storage of useful data is important. Here, the relational data model and SQL enable efficient and central storage of data that can then be used in collaboration or in separation by multiple departments. My understanding of these processes and their necessity was improved in this module.

Additionally, this module dived deeper into several security topics I previously had no experience in. API security measures and back-up procedures were explained, including practical exercises in units 10 and 11. As APIs present an additional way of receiving access to data, they must be compiled securely. Especially wherever personally identifiable data gets processed, APIs must be secure to defend against breaches. This connects to the necessity to comply with regulatory obligations like the GDPR that are also covered. E.g., a dedicated masking technique of sensitive data must be assembled to ensure even in the event of a data breach no sensitive information become accessible. This topic managed to increase my awareness for security topics. While I in a past career step worked as compliance officer for a German bank, diving deep into the obligations of the GDPR managed to provide me with a new perspective on working with data and enhanced my capabilities. Thinking about back-up procedures and comparing different options for composing data back-ups in unit 11 sparked curiosity to consider different interests within an organisation and how to handle them for me. While Data Engineers will typically aim to hold their pursuits manageable, they will also ensure that no significant amount of data is lost due to, e.g., service disruptions. Compliance Officers will ensure that data is stored as long as necessary due to regulations. These different roles and their different goals and perspectives must be managed closely in any successful operation which was reinforced by the multitude of different topics handled throughout the module.

Additionally to the important topics within big data analytics that were introduced throughout the module, I also learned about how to function as a team with different backgrounds and skills. In a group of three, we put together a logical database design. In a first meeting, we noticed that we had hugely different backgrounds. One person had no technical experience at all, but did a great job in putting together the requirements for our database of an online book store we chose to implement as well as in the critical evaluation of our final logical database design. Another person had great experience in the creation of ER-diagrams and thus handled this topic. As I have extensional experience in Cloud-based infrastructures, I put together our ideas about storage, cleaning and initial compliance considerations. We learned that it was helpful to first get an understanding of everyone’s strengths and weaknesses to then distribute the tasks evenly and in connection to the strengths of each team member. As I have some experience in managing teams due to my work as consultant, I took up the project management role. Overall, we were able to put together a strong first draft of the logical database design which I, in the unit 11 physical database design, only needed to tweak a little to provide the physical implementation of the database.

Concluding, throughout the module I was able to gain both technical and interpersonal skills. I will aim at implementing as many of these new skills as possible into my career and future tasks I work on in the domain of big data architectures and analytics.


## References:
Bhatnagar, A. & Gajjar, D. (2024) Policy implications of artificial intelligence.

Chen, W. (2023) Database Design and Implementation. ATU Faculty Open Educational Resources.

Connolly, T. M. & Begg, C. E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management. 6th ed. Essex: Pearson.

Cooksey, B. (2014) Real-Time Communication - An Introduction to APIs. Zapier.

Cove, V. (2020) Visualize COVID-19 Trends in ArcGIS Insights.

Datanami. (2019) Data Pipeline Automation: The Next Step Forward in DataOps.

Ethem, A. (2010) Introduction to Machine Learning. 2nd ed. The MIT Press.

Huxley et al. (2020) Data Cleaning. Sage Foundation.

IBM (2024) ACID properties of transactions.

IBM Intellas. (2016) Qradar and KAIF Integration Report.

IBM Intellas. (2010) What is a Database Management System?

Kazil, J & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly. Media Inc.

Letkowski, J. (2015) Doing Database Design with MySQL. Journal of Technology Research.

Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.

Taipalus, T. (2024) Database management system performance comparisons: A systematic literature review. The Journal of systems and software 208111872.

Williams, G. (2017) The Cybercitizen and Homeland Security. Freedom versus Fears. AICE Foundation.

Wired. (2020) What is GDPR? The summary guide to GDPR compliance in the UK.

Zoiner, T. (2024) Big data architectures.

